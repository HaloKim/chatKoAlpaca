{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatKoAlpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "국내 실력자분들이 한국어 LLM을 위해 좋은 오픈소스들을 공유해주셨습니다.\n",
    "\n",
    "한데모아 One-Step으로 chatGPT를 흉내낼 수 있게 작성하였습니다.\n",
    "\n",
    "작성한 환경은 A100 80G Single 입니다.\n",
    "\n",
    "torch 2.x 버전 이상을 colossalai에서 지원하지 않으므로 torch1.x 버전을 이용해주세요\n",
    "\n",
    "LLM은 polyglot-1.3B를 KoAlpaca_v1.1.json으로 추가학습하였으며 다른 LLM은 제 환경에서 OOM이 발생해\n",
    "좋은 환경이신분은 좋은 모델을 활용할 수 있을 것 같습니다.\n",
    "* 베이스 LLM https://github.com/Beomi/KoAlpaca\n",
    "* 베이스 Code https://github.com/airobotlab/KoChatGPT\n",
    "* 베이스 Data https://github.com/airobotlab/KoChatGPT/tree/main/data_kochatgpt\n",
    "\n",
    "오픈소스 공유에 감사인사 올립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb4KtOiFv4cw"
   },
   "source": [
    "# set Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyEBHY36ovQY",
    "outputId": "335d8682-1a97-44c1-c697-246ccc9f76ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sodvMed0qqwG",
    "outputId": "00dedb55-b961-45ba-def1-b8ee1f2a783e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt/kochatgpt_1_SFT.jsonl\n",
    "!wget https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt/kochatgpt_2_RM.jsonl\n",
    "!wget https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt/kochatgpt_3_PPO.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhutQamKv8AO",
    "tags": []
   },
   "source": [
    "# Step1) SFT(지도학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NDQOwP8pCfr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, AutoModelForCausalLM, pipeline, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "8fe0939433504673958736bd77ca3b59",
      "18e0f9550cec40a6b9ecf819c4807f6d",
      "644f578cdc0e424cbb2af32aa8e2615b",
      "7bdd205d9ea44b38a890ff006e90f5d9",
      "e1a18cb3fe8d423b890df252c1a9e599",
      "538a5792df47419296ca36504f6b40d6",
      "01e03037c3f74f6aafa140809ad78ef7",
      "57abd7cd0b3841f6bddc3a82da477b23",
      "d2ea0160be5f44c6920a03725b9ebac3",
      "c3266159dc3a41ba9c13cfb415e28cd2",
      "8818c4943b1c4a0d9466b32db0ea2d42",
      "4e85b4dd532c4d61adbef1fb0f110a7f",
      "64b52fa83f374aee8f4488a70314d142",
      "698e16ed0dc34b53bf509380c0b78f84",
      "b942d75f81ba490b887b1c6af9c8cb0a",
      "e33d51e99c2f4e4896931afc1f9e959d",
      "a1a26a1d1e9b4c8089e4a3d8ed5f2d21",
      "88e82b137d9147c993da6853be21d2cf",
      "ab55e2eec5a5468894348ccef1d63650",
      "e232bfd4c35642e39dc5757a11e632cc",
      "a08557311b0740d6817ed5a511eb2298",
      "532596024b1e44ae92935e6b82bc4c6b",
      "a3850b935744426a80da042883eef72e",
      "837d3d082ed049e29752b49aba9337b2",
      "91b5d937a4ff4f2385f74c43f320513f",
      "12f483b4282f45b8a72c7ff5d8cb71dd",
      "6fda5850786948b398c192315f83534a",
      "b91f0ff6a8e642fd9fad16f3dfcea708",
      "4778416a939b4caba8499487d8c245af",
      "d5327f3d60504843af3a6dd7343be3ac",
      "541355aca55c42d49cfd3339e93e5d7a",
      "e5e9abba2c124b7ea3c981da106f3765",
      "4ed1b179c0e54435af4bd2a9fba6ad43"
     ]
    },
    "id": "AbEkiE0wpDAn",
    "outputId": "3e6e4ace-a166-433b-b7ac-25be44c02e8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "print(tokenizer.tokenize(\"안녕하세요.\"))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=\"cuda\", non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8Im-PnupxXz",
    "outputId": "0c29c8f7-657c-4323-9946-6b3261a1b332",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=512,\n",
    "                         repetition_penalty=2.0,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAHr4EjspHlo",
    "outputId": "8eed60f3-bcd3-4204-d374-073e2a1caa81",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=202, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True,\n",
    ")\n",
    "generator(\n",
    "    [\"0 : **는 게임 좋아하니\\n1 :\",\n",
    "    \"0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 :\",\n",
    "    \"0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : \"],\n",
    "    **generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_j5h-LTtfkR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 모델 준비\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=256,\n",
    ")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0HlBf6VuVqf",
    "outputId": "f1fa2f89-f7c5-471e-bb6f-ec7f4073805a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "import logging\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_input = 'input'  # 내 데이터엔 input이 없다\n",
    "        pattern_output = 'completion'  # output\n",
    "\n",
    "        ############################################################\n",
    "        ## load dataset\n",
    "        # 내 데이터셋엔 input이 없다\n",
    "        # data_path_1_SFT = 'data_kochatgpt/korean_chatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "        # {'prompt': '불고기용 고기 한우에요?',\n",
    "        #  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
    "        #  'tokens': 193}        \n",
    "\n",
    "        ############################################################\n",
    "        ## 데이터셋 만들기, source와 target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "\n",
    "        # 입력\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # 출력\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        ############################################################\n",
    "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='./kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "eval_dataset = None  # eval은 안함\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "dfcE0o4nu5YP",
    "outputId": "3d3937c2-49e6-4d0b-b77d-f88e69fc7182",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved \n",
    "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    optim='adafactor',\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir='./output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_JBaZEvwCOj"
   },
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfFI_jmGvvPv",
    "outputId": "e6afea51-2714-481c-db3e-866903557294",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 추론 테스트\n",
    "custom_model = AutoModelForCausalLM.from_pretrained('./output_1_SFT').to('cuda')\n",
    "generator = pipeline('text-generation', model=custom_model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=202, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어',\n",
    "               '오늘 미세먼지 어때?']\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print(('#'*70))\n",
    "    print(('completion: %s' % (result[0]['generated_text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mniEYheFzcrp",
    "tags": []
   },
   "source": [
    "# Step2) RM(보상모델)\n",
    "\n",
    "세션 재시작 추천!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FQFW14awI6_",
    "outputId": "7590986e-b781-41e5-9d4e-4860461f1562",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "!git clone https://github.com/HaloKim/KoChatGPT.git\n",
    "\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install urllib3==1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOlDVA5_wI-A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3u98YSCwJAe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import AutoConfig, AutoModel, AutoConfig\n",
    "from chatgpt.models.base import RewardModel\n",
    "from torch import nn\n",
    "\n",
    "model_name = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT Reward model.\n",
    "    Args:\n",
    "        pretrained (str): Pretrained model name or path.\n",
    "        config (GPT2Config): Model config.\n",
    "        checkpoint (bool): Enable gradient checkpointing.\n",
    "        lora_rank (int): Rank of the low-rank approximation.\n",
    "        lora_train_bias (str): LoRA bias training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[AutoConfig] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = AutoModel.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n",
    "        elif config is not None:\n",
    "            model = AutoModel(config)\n",
    "        else:\n",
    "            model = AutoModel(AutoConfig())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "        # model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        value_head = nn.Linear(model.config.hidden_size, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        # 추가, 230421\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "    # 추가, 230421, config.json을 생성하기 위해 추가\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yu0MdWY-wJC7",
    "outputId": "2573adbd-7c28-43d1-8008-3221e79273d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "\n",
    "strategy = NaiveStrategy()\n",
    "lora_rank = 0\n",
    "pretrain = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "\n",
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrain, padding_side=\"right\", model_max_length=512)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "        }\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPTRM_custom(pretrained=pretrain, lora_rank=lora_rank, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_tx1pOOwJFr",
    "outputId": "c714e3f4-1329-4e81-b9d6-865f752168c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# make ranking data to chosen, rejetced data\n",
    "with open('./kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    if True:\n",
    "        print('## data check ##')\n",
    "        print((list_data_dict[0]))\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # data 1) 0 VS 1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    # data 2) 0 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # data 1) 1 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d' % (len(list_data_dict)))\n",
    "print('after  data num: %d' % (len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s' % total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-0vE6YY3RRK",
    "outputId": "49099f54-c751-4ac6-e537-3d613d9df5ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chatgpt.dataset import RewardDataset\n",
    "# prepare for data and dataset\n",
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 29000 학습\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 1000개만 평가\n",
    "\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3pkBJqw4NvW",
    "outputId": "5bb353a2-f02c-43b9-a99e-bf3e27b43bc8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from torch.optim import Adam\n",
    "\n",
    "optim = HybridAdam(model.parameters(), lr=5e-5)\n",
    "# optim = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l80C3q114izM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chatgpt.trainer import RewardModelTrainer\n",
    "\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=strategy,\n",
    "                             optim=optim,\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             max_epochs=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jObniVi-5fiX",
    "outputId": "af7157e4-b747-4f5a-f2bf-25e5658fdf8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# train!!\n",
    "trainer.fit(use_lora=lora_rank)\n",
    "\n",
    "\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "\n",
    "createDirectory('./output_2_RM')\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(model, os.path.join('./output_2_RM', 'RM.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(optim,\n",
    "                        os.path.join('./output_2_RM', 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)\n",
    "\n",
    "model.save_pretrained('./output_2_RM')  # config.json 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXSB9d-B6dll",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 보상모델 체크\n",
    "def inference_RM(input_text='인공지능은 인공지능 입니다'):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f' % (input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "\n",
    "# input_text = '한국은 대한민국 입니다'\n",
    "input_text = '인공지능은 인공지능 입니다'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkILPrZ-7c5B",
    "tags": []
   },
   "source": [
    "# Step3) PPO(강화학습)\n",
    "세션 재시작 추천!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install urllib3==1.26\n",
    "# !git clone https://github.com/hpcaitech/ColossalAI.git\n",
    "# %cd ColossalAI\n",
    "# !CUDA_EXT=1 pip install .\n",
    "# %cd ../\n",
    "!git clone https://github.com/HaloKim/KoChatGPT.git\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install colossalai==0.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:34.797870Z",
     "iopub.status.busy": "2023-05-26T01:27:34.796767Z",
     "iopub.status.idle": "2023-05-26T01:27:34.816962Z",
     "shell.execute_reply": "2023-05-26T01:27:34.815639Z",
     "shell.execute_reply.started": "2023-05-26T01:27:34.797812Z"
    },
    "id": "iVZxEyk57evI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "MAX_LEN = 256\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:37.618711Z",
     "iopub.status.busy": "2023-05-26T01:27:37.618088Z",
     "iopub.status.idle": "2023-05-26T01:28:35.064933Z",
     "shell.execute_reply": "2023-05-26T01:28:35.063243Z",
     "shell.execute_reply.started": "2023-05-26T01:27:37.618658Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
      "    registered at /opt/pytorch/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: Meta\n",
      "  previous kernel: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
      "       new kernel: registered at /dev/null:219 (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
      "  self.m.impl(name, dispatch_key, fn)\n"
     ]
    }
   ],
   "source": [
    "from chatgpt.models.auto import AutoActor, AutoCritic\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "\n",
    "# configure model, tokenizer\n",
    "strategy = NaiveStrategy()\n",
    "pretrain_actor = './output_1_SFT'  # SFT 모델 가져오기\n",
    "pretrain_critic = './output_2_RM'  # RM 모델 가져오기\n",
    "lora_rank = 0\n",
    "pretrain = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "\n",
    "with strategy.model_init_context():\n",
    "    actor = AutoActor(pretrained=pretrain_actor, lora_rank=lora_rank).to(torch.cuda.current_device())\n",
    "    critic = AutoCritic(pretrained=pretrain_critic, lora_rank=lora_rank).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrain, padding_side=\"right\", model_max_length=512)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "        }\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:35.154801Z",
     "iopub.status.busy": "2023-05-26T01:28:35.152302Z",
     "iopub.status.idle": "2023-05-26T01:28:37.148430Z",
     "shell.execute_reply": "2023-05-26T01:28:37.146478Z",
     "shell.execute_reply.started": "2023-05-26T01:28:35.154713Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[extension] Compiling or loading the JIT-built cpu_adam kernel during runtime now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emitting ninja build file /home/jovyan/.cache/colossalai/torch_extensions/torch1.13_cu11.8/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "[extension] Time to compile or load cpu_adam op: 0.36983585357666016 seconds\n",
      "False\n",
      "[extension] Compiling or loading the JIT-built fused_optim kernel during runtime now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/jovyan/.cache/colossalai/torch_extensions/torch1.13_cu11.8/build.ninja...\n",
      "Building extension module fused_optim...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_optim...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "[extension] Time to compile or load fused_optim op: 0.3223903179168701 seconds\n",
      "False\n",
      "[extension] Compiling or loading the JIT-built cpu_adam kernel during runtime now\n",
      "[extension] Time to compile or load cpu_adam op: 0.07686758041381836 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No modifications detected for re-loaded extension module cpu_adam, skipping build step...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[extension] Compiling or loading the JIT-built fused_optim kernel during runtime now\n",
      "[extension] Time to compile or load fused_optim op: 0.002943754196166992 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No modifications detected for re-loaded extension module fused_optim, skipping build step...\n",
      "Loading extension module fused_optim...\n"
     ]
    }
   ],
   "source": [
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from torch.optim import Adam\n",
    "\n",
    "actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
    "# actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "# critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:37.151855Z",
     "iopub.status.busy": "2023-05-26T01:28:37.151077Z",
     "iopub.status.idle": "2023-05-26T01:28:37.160406Z",
     "shell.execute_reply": "2023-05-26T01:28:37.158999Z",
     "shell.execute_reply.started": "2023-05-26T01:28:37.151793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting the models\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:37.164642Z",
     "iopub.status.busy": "2023-05-26T01:28:37.163767Z",
     "iopub.status.idle": "2023-05-26T01:28:37.200309Z",
     "shell.execute_reply": "2023-05-26T01:28:37.199356Z",
     "shell.execute_reply.started": "2023-05-26T01:28:37.164595Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "{'input_ids': tensor([[   44,  9883, 15741, 26661, 13384,   224, 16683,   224,  7245, 12063,\n",
      "         14445,  2479,    88,    91,  3628,  2423,    80,  2479,  3328,    17]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=MAX_LEN, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "\n",
    "# print(list_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(tokenize_fn('I want you to act as a linux terminal.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:37.201147Z",
     "iopub.status.busy": "2023-05-26T01:28:37.200954Z",
     "iopub.status.idle": "2023-05-26T01:29:47.354048Z",
     "shell.execute_reply": "2023-05-26T01:29:47.351129Z",
     "shell.execute_reply.started": "2023-05-26T01:28:37.201130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/1]:  67%|██████▋   | 2/3 [00:24<00:12, 12.21s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.78it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Episode [1/1]: 100%|██████████| 3/3 [00:37<00:00, 12.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "output_dir = './output_3_PPO'\n",
    "\n",
    "\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "\n",
    "createDirectory(output_dir)\n",
    "\n",
    "# configure trainer\n",
    "trainer = PPOTrainer(strategy,\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,\n",
    "                     train_batch_size=8,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=MAX_LEN,\n",
    "                     do_sample=True,\n",
    "                     temperature=1,\n",
    "                     top_k=50,\n",
    "                     top_p=0.9,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "## train!\n",
    "trainer.fit(list_prompt,  # 입력 prompt\n",
    "            num_episodes=1,\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(actor, os.path.join(output_dir, 'actor.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(actor_optim,\n",
    "                        os.path.join(output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:36:09.049162Z",
     "iopub.status.busy": "2023-05-26T01:36:09.047778Z",
     "iopub.status.idle": "2023-05-26T01:36:22.945065Z",
     "shell.execute_reply": "2023-05-26T01:36:22.944011Z",
     "shell.execute_reply.started": "2023-05-26T01:36:09.049103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 언어 모델로, 물리적 개념인 소고기와 돼지고기는 다른 종류이며, 불고그용 고기 역시 일반적으로 사용되는 종류가 아닙니다. 따라서, 질문에 대한 정확한 답변은 제공할 수 없습니다. 😊 😊 😊 😊죄송합니다. 😊하지만, 한우와 돼지고기를 사용하여 불고기를 만들 수 있는 방법이 있습니다. 일반적으로 불고기 요리를 할 때는 소고기를 사용하지만, 한우와 돼지고기도 불고기 요리에 사용될 수 있습니다. 이들 고기를 이용해 불고기와 된장\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨이 43대 부통령직을 수행한 년도는 1978년입니다.인 1978년 11월에 닉슨은 43대 부통령으로 지명되었습니다.당한 년도는 1978년입니다.은 닉슨의 업적을 인정하지 않는 일부 언론에 대한 탄압과 닉슨의 사생활과 관련된 문제를 제기하면서 부통령직에서 물러났습니다.은 닉슨의 재임기간 중 그의 업적을 인정하는 일부 언론에 대한 탄압과 사생활과 관련된 문제를 제기하면서 부통령직에서 물러났습니다.은 닉슨의 업적을\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 일리노이주 시카고 오헤어 국제공항에 위치해 있습니다.. 국제공항에서도 항공편이 있습니다.시카고 오헤어 국제공항은 미국 시카고에 위치한 공항으로, 시카고에서 약 3시간 정도 소요됩니다.시카고 오헤어 국제공항은 미국의 시카고에 위치한 국제공항으로, 시카고에서 약 1시간 정도 소요됩니다.시카고 오헤어 국제공항은 미국 시카고에서 3시간 정도 소요됩니다.시카고 오헤어 국제공항은 미국 시카고 오헤어 국제공항에서 약 30분 정도 소요됩니다.시카고 오헤어 국제공항은 미국 일리노이주 시카고에 위치해 있\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 미세먼지 농도가 어떻게 발생하는 지에 대한 정보를 알지 못합니다. 하지만 일반적으로 미세먼지 농도는 지역에 따라 다르며, 특히 황사나 미세먼지 등의 오염 물질은 중국발 미세먼지 등 다양한 요인에 의해 발생할 수 있습니다. 따라서 해당 지역의 기상 예보나 대기 상태를 참고하시는 것이 좋습니다.... 예보, 기상특보 등을 확인하여 대응하시기 바랍니다.. 👌.. 예보, 기상특보 등은 인터넷이나 해당 지역의 기상청 등에서 확인하실 수 있습니다.. 👌\n"
     ]
    }
   ],
   "source": [
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=MAX_LEN,\n",
    "                             do_sample=True,\n",
    "                             top_k=20,\n",
    "                             top_p=0.95,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             repetition_penalty=2.0,\n",
    "                             num_return_sequences=1,\n",
    "                             early_stopping=True)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UaKZziI9x4-",
    "tags": []
   },
   "source": [
    "# Final) Test !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:38:10.816942Z",
     "iopub.status.busy": "2023-05-26T01:38:10.815444Z",
     "iopub.status.idle": "2023-05-26T01:38:10.828619Z",
     "shell.execute_reply": "2023-05-26T01:38:10.827364Z",
     "shell.execute_reply.started": "2023-05-26T01:38:10.816877Z"
    },
    "id": "LZRctQyl-0Lb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "from chatgpt.models.auto import AutoActor, AutoCritic\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "MAX_LEN = 256\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\":\n",
    "    (\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "     \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "     \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "     ),\n",
    "    \"prompt_no_input\":\n",
    "    (\"Below is an instruction that describes a task.\\n\"\n",
    "     \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "     \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-26T01:37:37.209772Z",
     "iopub.status.busy": "2023-05-26T01:37:37.208843Z",
     "iopub.status.idle": "2023-05-26T01:38:03.118836Z",
     "shell.execute_reply": "2023-05-26T01:38:03.117765Z",
     "shell.execute_reply.started": "2023-05-26T01:37:37.209704Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoActor(\n",
       "  (model): GPTNeoXForCausalLM(\n",
       "    (gpt_neox): GPTNeoXModel(\n",
       "      (embed_in): Embedding(30080, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (1): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (2): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (3): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (4): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (5): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (6): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (7): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (8): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (9): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (10): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (11): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (12): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (13): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (14): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (15): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (16): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (17): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (18): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (19): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (20): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (21): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (22): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (23): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pretrain = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "model_directory = './output_3_PPO'\n",
    "model_path = os.path.join(model_directory, 'actor.pt')\n",
    "\n",
    "# configure model, tokenizer\n",
    "actor = AutoActor(pretrained=pretrain).to(torch.cuda.current_device())\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain,\n",
    "                                          padding_side=\"right\",\n",
    "                                          model_max_length=512)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "    \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "    \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "actor.model.load_state_dict(state_dict)\n",
    "\n",
    "actor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:38:16.157554Z",
     "iopub.status.busy": "2023-05-26T01:38:16.156408Z",
     "iopub.status.idle": "2023-05-26T01:38:31.544682Z",
     "shell.execute_reply": "2023-05-26T01:38:31.543770Z",
     "shell.execute_reply.started": "2023-05-26T01:38:16.157496Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 01:38:20.279311: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-26 01:38:20.319378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'제가 AI 챗봇으로 프로그래밍되어있기 때문에 물리적인 정보를 제공할 수는 없습니다. 하지만 한우는 한우 중에서도 고급육종으로 유명하며, 그 맛과 영양적 가치가 높은 고급 육류로 인정받고 있습니다.에 따라 가격이 다를 수 있지만 일반적으로 한우는 한우 중에서 가장 비싼 가격대에 속한답니다. 따라서 구매 전에는 가격 정보를 충분히 확인해보시는 것이 좋습니다.!다사오지 마세요.이의 답변을 제공해드리겠습니다. :\\n\\n'네, 한우 중에서도 고급\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 1973년부터 1974년까지 부통령을 역임했습니다.\\n\\n하지만 리처드 닉슨이 43대 부통령을 수행한 정확한 년도는 알려지지 않았습니다.닉슨이 43대 부통령을 수행한 해는 1944년이었습니다.닉슨이 43대 부통령을 수행한 해는 정확히 알려지지 않았지만, 미국 내에서는 1950년대에 이르러 이 문제에 대한 연구가 시작되었습니다.닉슨의 이름이 누구인지에 대한 정보는 제공되지 않았습니다.닉슨은\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 일리노이 주 시카고에 위치해 있습니다. 국제공항은 시카고에서 북쪽으로 1시간 정도의 거리에 위치해 있습니다. 국제공항은 시카고에서 동쪽으로 1시간 정도의 거리에 위치해 있습니다. 국제공항은 시카고에서 남쪽으로 3시간 정도의 거리에 위치해 있습니다. 국제공항은 시카고와 인접한 곳에 위치해 있습니다. 국제공항은 시카고에서 서쪽으로 약 1시간 정도의 거리에 위치해 있습니다. 국제공항은 시카고와 남쪽으로 3시간 정도의 거리에 위치해 있습니다. 국제공항은 시카고에서 북쪽으로 1시간 정도의\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):' 미세먼지 농도는 전국적으로 매우 심각한 수준이며, 마스크 착용과 실외 활동 자제 등을 권장하고 있습니다. 외출 시에는 미세먼지 마스크 착용이 필수적입니다. 미세먼지 농도는 미세먼지 농도와는 상관없이 매우 심각한 수준입니다.\\n 미세먼지 농도를 확인하고 마스크를 착용하는 것이 좋습니다. 미세먼지 농도에 대해서는 정확한 정보를 찾기 어렵습니다. 따라서 미세먼지 농도를 확인하고, 미세먼지 마스크를 착용하는 것이 좋습니다. 미세먼지 농도를 확인하려면 미세먼지 농도 측정기로 측정하거나, 미세먼지 농도 웹사이트를 통해 확인할 수 있습니다. 미세먼지\n"
     ]
    }
   ],
   "source": [
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=MAX_LEN,\n",
    "                             do_sample=True,\n",
    "                             top_k=20,\n",
    "                             top_p=0.95,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             repetition_penalty=2.0,\n",
    "                             num_return_sequences=1,\n",
    "                             early_stopping=True)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01e03037c3f74f6aafa140809ad78ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12f483b4282f45b8a72c7ff5d8cb71dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5e9abba2c124b7ea3c981da106f3765",
      "placeholder": "​",
      "style": "IPY_MODEL_4ed1b179c0e54435af4bd2a9fba6ad43",
      "value": " 513M/513M [00:03&lt;00:00, 168MB/s]"
     }
    },
    "18e0f9550cec40a6b9ecf819c4807f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_538a5792df47419296ca36504f6b40d6",
      "placeholder": "​",
      "style": "IPY_MODEL_01e03037c3f74f6aafa140809ad78ef7",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "4778416a939b4caba8499487d8c245af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e85b4dd532c4d61adbef1fb0f110a7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64b52fa83f374aee8f4488a70314d142",
       "IPY_MODEL_698e16ed0dc34b53bf509380c0b78f84",
       "IPY_MODEL_b942d75f81ba490b887b1c6af9c8cb0a"
      ],
      "layout": "IPY_MODEL_e33d51e99c2f4e4896931afc1f9e959d"
     }
    },
    "4ed1b179c0e54435af4bd2a9fba6ad43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "532596024b1e44ae92935e6b82bc4c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "538a5792df47419296ca36504f6b40d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "541355aca55c42d49cfd3339e93e5d7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57abd7cd0b3841f6bddc3a82da477b23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "644f578cdc0e424cbb2af32aa8e2615b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57abd7cd0b3841f6bddc3a82da477b23",
      "max": 2825034,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2ea0160be5f44c6920a03725b9ebac3",
      "value": 2825034
     }
    },
    "64b52fa83f374aee8f4488a70314d142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1a26a1d1e9b4c8089e4a3d8ed5f2d21",
      "placeholder": "​",
      "style": "IPY_MODEL_88e82b137d9147c993da6853be21d2cf",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "698e16ed0dc34b53bf509380c0b78f84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab55e2eec5a5468894348ccef1d63650",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e232bfd4c35642e39dc5757a11e632cc",
      "value": 1000
     }
    },
    "6fda5850786948b398c192315f83534a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bdd205d9ea44b38a890ff006e90f5d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3266159dc3a41ba9c13cfb415e28cd2",
      "placeholder": "​",
      "style": "IPY_MODEL_8818c4943b1c4a0d9466b32db0ea2d42",
      "value": " 2.83M/2.83M [00:00&lt;00:00, 6.58MB/s]"
     }
    },
    "837d3d082ed049e29752b49aba9337b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b91f0ff6a8e642fd9fad16f3dfcea708",
      "placeholder": "​",
      "style": "IPY_MODEL_4778416a939b4caba8499487d8c245af",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "8818c4943b1c4a0d9466b32db0ea2d42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88e82b137d9147c993da6853be21d2cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fe0939433504673958736bd77ca3b59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18e0f9550cec40a6b9ecf819c4807f6d",
       "IPY_MODEL_644f578cdc0e424cbb2af32aa8e2615b",
       "IPY_MODEL_7bdd205d9ea44b38a890ff006e90f5d9"
      ],
      "layout": "IPY_MODEL_e1a18cb3fe8d423b890df252c1a9e599"
     }
    },
    "91b5d937a4ff4f2385f74c43f320513f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5327f3d60504843af3a6dd7343be3ac",
      "max": 513302779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_541355aca55c42d49cfd3339e93e5d7a",
      "value": 513302779
     }
    },
    "a08557311b0740d6817ed5a511eb2298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a26a1d1e9b4c8089e4a3d8ed5f2d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3850b935744426a80da042883eef72e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_837d3d082ed049e29752b49aba9337b2",
       "IPY_MODEL_91b5d937a4ff4f2385f74c43f320513f",
       "IPY_MODEL_12f483b4282f45b8a72c7ff5d8cb71dd"
      ],
      "layout": "IPY_MODEL_6fda5850786948b398c192315f83534a"
     }
    },
    "ab55e2eec5a5468894348ccef1d63650": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b91f0ff6a8e642fd9fad16f3dfcea708": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b942d75f81ba490b887b1c6af9c8cb0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a08557311b0740d6817ed5a511eb2298",
      "placeholder": "​",
      "style": "IPY_MODEL_532596024b1e44ae92935e6b82bc4c6b",
      "value": " 1.00k/1.00k [00:00&lt;00:00, 15.7kB/s]"
     }
    },
    "c3266159dc3a41ba9c13cfb415e28cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2ea0160be5f44c6920a03725b9ebac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5327f3d60504843af3a6dd7343be3ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1a18cb3fe8d423b890df252c1a9e599": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e232bfd4c35642e39dc5757a11e632cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e33d51e99c2f4e4896931afc1f9e959d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5e9abba2c124b7ea3c981da106f3765": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
