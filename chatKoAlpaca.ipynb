{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatKoAlpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "êµ­ë‚´ ì‹¤ë ¥ìë¶„ë“¤ì´ í•œêµ­ì–´ LLMì„ ìœ„í•´ ì¢‹ì€ ì˜¤í”ˆì†ŒìŠ¤ë“¤ì„ ê³µìœ í•´ì£¼ì…¨ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•œë°ëª¨ì•„ One-Stepìœ¼ë¡œ chatGPTë¥¼ í‰ë‚´ë‚¼ ìˆ˜ ìˆê²Œ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‘ì„±í•œ í™˜ê²½ì€ A100 80G Single ì…ë‹ˆë‹¤.\n",
    "\n",
    "torch 2.x ë²„ì „ ì´ìƒì„ colossalaiì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ torch1.x ë²„ì „ì„ ì´ìš©í•´ì£¼ì„¸ìš”\n",
    "\n",
    "LLMì€ polyglot-1.3Bë¥¼ KoAlpaca_v1.1.jsonìœ¼ë¡œ ì¶”ê°€í•™ìŠµí•˜ì˜€ìœ¼ë©° ë‹¤ë¥¸ LLMì€ ì œ í™˜ê²½ì—ì„œ OOMì´ ë°œìƒí•´\n",
    "ì¢‹ì€ í™˜ê²½ì´ì‹ ë¶„ì€ ì¢‹ì€ ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "* ë² ì´ìŠ¤ LLM https://github.com/Beomi/KoAlpaca\n",
    "* ë² ì´ìŠ¤ Code https://github.com/airobotlab/KoChatGPT\n",
    "* ë² ì´ìŠ¤ Data https://github.com/airobotlab/KoChatGPT/tree/main/data_kochatgpt\n",
    "\n",
    "ì˜¤í”ˆì†ŒìŠ¤ ê³µìœ ì— ê°ì‚¬ì¸ì‚¬ ì˜¬ë¦½ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb4KtOiFv4cw"
   },
   "source": [
    "# set Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyEBHY36ovQY",
    "outputId": "335d8682-1a97-44c1-c697-246ccc9f76ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sodvMed0qqwG",
    "outputId": "00dedb55-b961-45ba-def1-b8ee1f2a783e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt/kochatgpt_1_SFT.jsonl\n",
    "!wget https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt/kochatgpt_2_RM.jsonl\n",
    "!wget https://raw.githubusercontent.com/airobotlab/KoChatGPT/main/data_kochatgpt/kochatgpt_3_PPO.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhutQamKv8AO",
    "tags": []
   },
   "source": [
    "# Step1) SFT(ì§€ë„í•™ìŠµ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NDQOwP8pCfr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, AutoModelForCausalLM, pipeline, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "8fe0939433504673958736bd77ca3b59",
      "18e0f9550cec40a6b9ecf819c4807f6d",
      "644f578cdc0e424cbb2af32aa8e2615b",
      "7bdd205d9ea44b38a890ff006e90f5d9",
      "e1a18cb3fe8d423b890df252c1a9e599",
      "538a5792df47419296ca36504f6b40d6",
      "01e03037c3f74f6aafa140809ad78ef7",
      "57abd7cd0b3841f6bddc3a82da477b23",
      "d2ea0160be5f44c6920a03725b9ebac3",
      "c3266159dc3a41ba9c13cfb415e28cd2",
      "8818c4943b1c4a0d9466b32db0ea2d42",
      "4e85b4dd532c4d61adbef1fb0f110a7f",
      "64b52fa83f374aee8f4488a70314d142",
      "698e16ed0dc34b53bf509380c0b78f84",
      "b942d75f81ba490b887b1c6af9c8cb0a",
      "e33d51e99c2f4e4896931afc1f9e959d",
      "a1a26a1d1e9b4c8089e4a3d8ed5f2d21",
      "88e82b137d9147c993da6853be21d2cf",
      "ab55e2eec5a5468894348ccef1d63650",
      "e232bfd4c35642e39dc5757a11e632cc",
      "a08557311b0740d6817ed5a511eb2298",
      "532596024b1e44ae92935e6b82bc4c6b",
      "a3850b935744426a80da042883eef72e",
      "837d3d082ed049e29752b49aba9337b2",
      "91b5d937a4ff4f2385f74c43f320513f",
      "12f483b4282f45b8a72c7ff5d8cb71dd",
      "6fda5850786948b398c192315f83534a",
      "b91f0ff6a8e642fd9fad16f3dfcea708",
      "4778416a939b4caba8499487d8c245af",
      "d5327f3d60504843af3a6dd7343be3ac",
      "541355aca55c42d49cfd3339e93e5d7a",
      "e5e9abba2c124b7ea3c981da106f3765",
      "4ed1b179c0e54435af4bd2a9fba6ad43"
     ]
    },
    "id": "AbEkiE0wpDAn",
    "outputId": "3e6e4ace-a166-433b-b7ac-25be44c02e8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”.\"))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=\"cuda\", non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8Im-PnupxXz",
    "outputId": "0c29c8f7-657c-4323-9946-6b3261a1b332",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=512,\n",
    "                         repetition_penalty=2.0,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAHr4EjspHlo",
    "outputId": "8eed60f3-bcd3-4204-d374-073e2a1caa81",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=202, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True,\n",
    ")\n",
    "generator(\n",
    "    [\"0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\\n1 :\",\n",
    "    \"0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\\n1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\\n0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\\n1 :\",\n",
    "    \"0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\\n1 : ë­” ì¼ ìˆì—ˆì–´?\\n0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\\n1 : \"],\n",
    "    **generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_j5h-LTtfkR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ëª¨ë¸ ì¤€ë¹„\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=256,\n",
    ")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0HlBf6VuVqf",
    "outputId": "f1fa2f89-f7c5-471e-bb6f-ec7f4073805a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "import logging\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_input = 'input'  # ë‚´ ë°ì´í„°ì—” inputì´ ì—†ë‹¤\n",
    "        pattern_output = 'completion'  # output\n",
    "\n",
    "        ############################################################\n",
    "        ## load dataset\n",
    "        # ë‚´ ë°ì´í„°ì…‹ì—” inputì´ ì—†ë‹¤\n",
    "        # data_path_1_SFT = 'data_kochatgpt/korean_chatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "        # {'prompt': 'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "        #  'completion': \"'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë©°, ì§ì ‘ì ìœ¼ë¡œ ì‹í’ˆì— ê´€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” í•œìš°, ì‡ ê³ ê¸°, ë¼ì§€ê³ ê¸° ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•œìš°ëŠ” ëŒ€í‘œì ì¸ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì—, í•œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ì•ŒëŸ¬ì§€ë‚˜ ê°œë³„ ê±´ê°• ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ í›„ì— ì„ íƒí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\",\n",
    "        #  'tokens': 193}        \n",
    "\n",
    "        ############################################################\n",
    "        ## ë°ì´í„°ì…‹ ë§Œë“¤ê¸°, sourceì™€ target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "        # ì…ë ¥\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # ì¶œë ¥\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        ############################################################\n",
    "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # sourceë§Œ\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## ì…ë ¥ì€ source, ì¶œë ¥ì€ source+target ì´ì§€ë§Œ í•™ìŠµì€ target ë¶€ë¶„ë§Œ\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source ë¶€ë¶„ì€ -100ìœ¼ë¡œ ì±„ìš´ë‹¤\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='./kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "eval_dataset = None  # evalì€ ì•ˆí•¨\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "dfcE0o4nu5YP",
    "outputId": "3d3937c2-49e6-4d0b-b77d-f88e69fc7182",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "# training_args ìˆ˜ì • ê°€ëŠ¥: https://github.com/Beomi/KoAlpaca/blob/main/train.sh ì°¸ê³ \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved \n",
    "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    optim='adafactor',\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir='./output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_JBaZEvwCOj"
   },
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfFI_jmGvvPv",
    "outputId": "e6afea51-2714-481c-db3e-866903557294",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "custom_model = AutoModelForCausalLM.from_pretrained('./output_1_SFT').to('cuda')\n",
    "generator = pipeline('text-generation', model=custom_model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=202, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print(('#'*70))\n",
    "    print(('completion: %s' % (result[0]['generated_text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mniEYheFzcrp",
    "tags": []
   },
   "source": [
    "# Step2) RM(ë³´ìƒëª¨ë¸)\n",
    "\n",
    "ì„¸ì…˜ ì¬ì‹œì‘ ì¶”ì²œ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FQFW14awI6_",
    "outputId": "7590986e-b781-41e5-9d4e-4860461f1562",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "!git clone https://github.com/HaloKim/KoChatGPT.git\n",
    "\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install urllib3==1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOlDVA5_wI-A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3u98YSCwJAe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import AutoConfig, AutoModel, AutoConfig\n",
    "from chatgpt.models.base import RewardModel\n",
    "from torch import nn\n",
    "\n",
    "model_name = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT Reward model.\n",
    "    Args:\n",
    "        pretrained (str): Pretrained model name or path.\n",
    "        config (GPT2Config): Model config.\n",
    "        checkpoint (bool): Enable gradient checkpointing.\n",
    "        lora_rank (int): Rank of the low-rank approximation.\n",
    "        lora_train_bias (str): LoRA bias training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[AutoConfig] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = AutoModel.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n",
    "        elif config is not None:\n",
    "            model = AutoModel(config)\n",
    "        else:\n",
    "            model = AutoModel(AutoConfig())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "        # model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        value_head = nn.Linear(model.config.hidden_size, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        # ì¶”ê°€, 230421\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "    # ì¶”ê°€, 230421, config.jsonì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì¶”ê°€\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yu0MdWY-wJC7",
    "outputId": "2573adbd-7c28-43d1-8008-3221e79273d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "\n",
    "strategy = NaiveStrategy()\n",
    "lora_rank = 0\n",
    "pretrain = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "\n",
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrain, padding_side=\"right\", model_max_length=512)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "        }\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPTRM_custom(pretrained=pretrain, lora_rank=lora_rank, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_tx1pOOwJFr",
    "outputId": "c714e3f4-1329-4e81-b9d6-865f752168c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# make ranking data to chosen, rejetced data\n",
    "with open('./kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    if True:\n",
    "        print('## data check ##')\n",
    "        print((list_data_dict[0]))\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # data 1) 0 VS 1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    # data 2) 0 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # data 1) 1 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d' % (len(list_data_dict)))\n",
    "print('after  data num: %d' % (len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s' % total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-0vE6YY3RRK",
    "outputId": "49099f54-c751-4ac6-e537-3d613d9df5ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chatgpt.dataset import RewardDataset\n",
    "# prepare for data and dataset\n",
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 29000 í•™ìŠµ\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 1000ê°œë§Œ í‰ê°€\n",
    "\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3pkBJqw4NvW",
    "outputId": "5bb353a2-f02c-43b9-a99e-bf3e27b43bc8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from torch.optim import Adam\n",
    "\n",
    "optim = HybridAdam(model.parameters(), lr=5e-5)\n",
    "# optim = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l80C3q114izM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chatgpt.trainer import RewardModelTrainer\n",
    "\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=strategy,\n",
    "                             optim=optim,\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=1,\n",
    "                             max_epochs=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jObniVi-5fiX",
    "outputId": "af7157e4-b747-4f5a-f2bf-25e5658fdf8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# train!!\n",
    "trainer.fit(use_lora=lora_rank)\n",
    "\n",
    "\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "\n",
    "createDirectory('./output_2_RM')\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(model, os.path.join('./output_2_RM', 'RM.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(optim,\n",
    "                        os.path.join('./output_2_RM', 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)\n",
    "\n",
    "model.save_pretrained('./output_2_RM')  # config.json ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXSB9d-B6dll",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ë³´ìƒëª¨ë¸ ì²´í¬\n",
    "def inference_RM(input_text='ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f' % (input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "\n",
    "# input_text = 'í•œêµ­ì€ ëŒ€í•œë¯¼êµ­ ì…ë‹ˆë‹¤'\n",
    "input_text = 'ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkILPrZ-7c5B",
    "tags": []
   },
   "source": [
    "# Step3) PPO(ê°•í™”í•™ìŠµ)\n",
    "ì„¸ì…˜ ì¬ì‹œì‘ ì¶”ì²œ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install urllib3==1.26\n",
    "# !git clone https://github.com/hpcaitech/ColossalAI.git\n",
    "# %cd ColossalAI\n",
    "# !CUDA_EXT=1 pip install .\n",
    "# %cd ../\n",
    "!git clone https://github.com/HaloKim/KoChatGPT.git\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install colossalai==0.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:34.797870Z",
     "iopub.status.busy": "2023-05-26T01:27:34.796767Z",
     "iopub.status.idle": "2023-05-26T01:27:34.816962Z",
     "shell.execute_reply": "2023-05-26T01:27:34.815639Z",
     "shell.execute_reply.started": "2023-05-26T01:27:34.797812Z"
    },
    "id": "iVZxEyk57evI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "MAX_LEN = 256\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:37.618711Z",
     "iopub.status.busy": "2023-05-26T01:27:37.618088Z",
     "iopub.status.idle": "2023-05-26T01:28:35.064933Z",
     "shell.execute_reply": "2023-05-26T01:28:35.063243Z",
     "shell.execute_reply.started": "2023-05-26T01:27:37.618658Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
      "    registered at /opt/pytorch/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: Meta\n",
      "  previous kernel: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
      "       new kernel: registered at /dev/null:219 (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
      "  self.m.impl(name, dispatch_key, fn)\n"
     ]
    }
   ],
   "source": [
    "from chatgpt.models.auto import AutoActor, AutoCritic\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "\n",
    "# configure model, tokenizer\n",
    "strategy = NaiveStrategy()\n",
    "pretrain_actor = './output_1_SFT'  # SFT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "pretrain_critic = './output_2_RM'  # RM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "lora_rank = 0\n",
    "pretrain = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "\n",
    "with strategy.model_init_context():\n",
    "    actor = AutoActor(pretrained=pretrain_actor, lora_rank=lora_rank).to(torch.cuda.current_device())\n",
    "    critic = AutoCritic(pretrained=pretrain_critic, lora_rank=lora_rank).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrain, padding_side=\"right\", model_max_length=512)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "        }\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:35.154801Z",
     "iopub.status.busy": "2023-05-26T01:28:35.152302Z",
     "iopub.status.idle": "2023-05-26T01:28:37.148430Z",
     "shell.execute_reply": "2023-05-26T01:28:37.146478Z",
     "shell.execute_reply.started": "2023-05-26T01:28:35.154713Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[extension] Compiling or loading the JIT-built cpu_adam kernel during runtime now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emitting ninja build file /home/jovyan/.cache/colossalai/torch_extensions/torch1.13_cu11.8/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "[extension] Time to compile or load cpu_adam op: 0.36983585357666016 seconds\n",
      "False\n",
      "[extension] Compiling or loading the JIT-built fused_optim kernel during runtime now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/jovyan/.cache/colossalai/torch_extensions/torch1.13_cu11.8/build.ninja...\n",
      "Building extension module fused_optim...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_optim...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "[extension] Time to compile or load fused_optim op: 0.3223903179168701 seconds\n",
      "False\n",
      "[extension] Compiling or loading the JIT-built cpu_adam kernel during runtime now\n",
      "[extension] Time to compile or load cpu_adam op: 0.07686758041381836 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No modifications detected for re-loaded extension module cpu_adam, skipping build step...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[extension] Compiling or loading the JIT-built fused_optim kernel during runtime now\n",
      "[extension] Time to compile or load fused_optim op: 0.002943754196166992 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No modifications detected for re-loaded extension module fused_optim, skipping build step...\n",
      "Loading extension module fused_optim...\n"
     ]
    }
   ],
   "source": [
    "from colossalai.nn.optimizer import HybridAdam\n",
    "from torch.optim import Adam\n",
    "\n",
    "actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
    "# actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "# critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:37.151855Z",
     "iopub.status.busy": "2023-05-26T01:28:37.151077Z",
     "iopub.status.idle": "2023-05-26T01:28:37.160406Z",
     "shell.execute_reply": "2023-05-26T01:28:37.158999Z",
     "shell.execute_reply.started": "2023-05-26T01:28:37.151793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting the models\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:37.164642Z",
     "iopub.status.busy": "2023-05-26T01:28:37.163767Z",
     "iopub.status.idle": "2023-05-26T01:28:37.200309Z",
     "shell.execute_reply": "2023-05-26T01:28:37.199356Z",
     "shell.execute_reply.started": "2023-05-26T01:28:37.164595Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "{'input_ids': tensor([[   44,  9883, 15741, 26661, 13384,   224, 16683,   224,  7245, 12063,\n",
      "         14445,  2479,    88,    91,  3628,  2423,    80,  2479,  3328,    17]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=MAX_LEN, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "\n",
    "# print(list_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(tokenize_fn('I want you to act as a linux terminal.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:28:37.201147Z",
     "iopub.status.busy": "2023-05-26T01:28:37.200954Z",
     "iopub.status.idle": "2023-05-26T01:29:47.354048Z",
     "shell.execute_reply": "2023-05-26T01:29:47.351129Z",
     "shell.execute_reply.started": "2023-05-26T01:28:37.201130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:24<00:12, 12.21s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.78it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.78it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Train epoch [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.91it/s, actor_loss=0, critic_loss=0]\u001b[A\n",
      "Episode [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:37<00:00, 12.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "output_dir = './output_3_PPO'\n",
    "\n",
    "\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "\n",
    "createDirectory(output_dir)\n",
    "\n",
    "# configure trainer\n",
    "trainer = PPOTrainer(strategy,\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,\n",
    "                     train_batch_size=8,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=MAX_LEN,\n",
    "                     do_sample=True,\n",
    "                     temperature=1,\n",
    "                     top_k=50,\n",
    "                     top_p=0.9,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "## train!\n",
    "trainer.fit(list_prompt,  # ì…ë ¥ prompt\n",
    "            num_episodes=1,\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(actor, os.path.join(output_dir, 'actor.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(actor_optim,\n",
    "                        os.path.join(output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:36:09.049162Z",
     "iopub.status.busy": "2023-05-26T01:36:09.047778Z",
     "iopub.status.idle": "2023-05-26T01:36:22.945065Z",
     "shell.execute_reply": "2023-05-26T01:36:22.944011Z",
     "shell.execute_reply.started": "2023-05-26T01:36:09.049103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–¸ì–´ ëª¨ë¸ë¡œ, ë¬¼ë¦¬ì  ê°œë…ì¸ ì†Œê³ ê¸°ì™€ ë¼ì§€ê³ ê¸°ëŠ” ë‹¤ë¥¸ ì¢…ë¥˜ì´ë©°, ë¶ˆê³ ê·¸ìš© ê³ ê¸° ì—­ì‹œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì¢…ë¥˜ê°€ ì•„ë‹™ë‹ˆë‹¤. ë”°ë¼ì„œ, ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì€ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜Š ğŸ˜Š ğŸ˜Š ğŸ˜Šì£„ì†¡í•©ë‹ˆë‹¤. ğŸ˜Ší•˜ì§€ë§Œ, í•œìš°ì™€ ë¼ì§€ê³ ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆê³ ê¸°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸° ìš”ë¦¬ë¥¼ í•  ë•ŒëŠ” ì†Œê³ ê¸°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, í•œìš°ì™€ ë¼ì§€ê³ ê¸°ë„ ë¶ˆê³ ê¸° ìš”ë¦¬ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ ê³ ê¸°ë¥¼ ì´ìš©í•´ ë¶ˆê³ ê¸°ì™€ ëœì¥\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ” 1978ë…„ì…ë‹ˆë‹¤.ì¸ 1978ë…„ 11ì›”ì— ë‹‰ìŠ¨ì€ 43ëŒ€ ë¶€í†µë ¹ìœ¼ë¡œ ì§€ëª…ë˜ì—ˆìŠµë‹ˆë‹¤.ë‹¹í•œ ë…„ë„ëŠ” 1978ë…„ì…ë‹ˆë‹¤.ì€ ë‹‰ìŠ¨ì˜ ì—…ì ì„ ì¸ì •í•˜ì§€ ì•ŠëŠ” ì¼ë¶€ ì–¸ë¡ ì— ëŒ€í•œ íƒ„ì••ê³¼ ë‹‰ìŠ¨ì˜ ì‚¬ìƒí™œê³¼ ê´€ë ¨ëœ ë¬¸ì œë¥¼ ì œê¸°í•˜ë©´ì„œ ë¶€í†µë ¹ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ìŠµë‹ˆë‹¤.ì€ ë‹‰ìŠ¨ì˜ ì¬ì„ê¸°ê°„ ì¤‘ ê·¸ì˜ ì—…ì ì„ ì¸ì •í•˜ëŠ” ì¼ë¶€ ì–¸ë¡ ì— ëŒ€í•œ íƒ„ì••ê³¼ ì‚¬ìƒí™œê³¼ ê´€ë ¨ëœ ë¬¸ì œë¥¼ ì œê¸°í•˜ë©´ì„œ ë¶€í†µë ¹ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ìŠµë‹ˆë‹¤.ì€ ë‹‰ìŠ¨ì˜ ì—…ì ì„\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.. êµ­ì œê³µí•­ì—ì„œë„ í•­ê³µí¸ì´ ìˆìŠµë‹ˆë‹¤.ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•œ ê³µí•­ìœ¼ë¡œ, ì‹œì¹´ê³ ì—ì„œ ì•½ 3ì‹œê°„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ì˜ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•œ êµ­ì œê³µí•­ìœ¼ë¡œ, ì‹œì¹´ê³ ì—ì„œ ì•½ 1ì‹œê°„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì‹œì¹´ê³ ì—ì„œ 3ì‹œê°„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì—ì„œ ì•½ 30ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•´ ìˆ\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë¯€ë¡œ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ê°€ ì–´ë–»ê²Œ ë°œìƒí•˜ëŠ” ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œì§€ ëª»í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” ì§€ì—­ì— ë”°ë¼ ë‹¤ë¥´ë©°, íŠ¹íˆ í™©ì‚¬ë‚˜ ë¯¸ì„¸ë¨¼ì§€ ë“±ì˜ ì˜¤ì—¼ ë¬¼ì§ˆì€ ì¤‘êµ­ë°œ ë¯¸ì„¸ë¨¼ì§€ ë“± ë‹¤ì–‘í•œ ìš”ì¸ì— ì˜í•´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹ ì§€ì—­ì˜ ê¸°ìƒ ì˜ˆë³´ë‚˜ ëŒ€ê¸° ìƒíƒœë¥¼ ì°¸ê³ í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.... ì˜ˆë³´, ê¸°ìƒíŠ¹ë³´ ë“±ì„ í™•ì¸í•˜ì—¬ ëŒ€ì‘í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.. ğŸ‘Œ.. ì˜ˆë³´, ê¸°ìƒíŠ¹ë³´ ë“±ì€ ì¸í„°ë„·ì´ë‚˜ í•´ë‹¹ ì§€ì—­ì˜ ê¸°ìƒì²­ ë“±ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.. ğŸ‘Œ\n"
     ]
    }
   ],
   "source": [
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=MAX_LEN,\n",
    "                             do_sample=True,\n",
    "                             top_k=20,\n",
    "                             top_p=0.95,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             repetition_penalty=2.0,\n",
    "                             num_return_sequences=1,\n",
    "                             early_stopping=True)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UaKZziI9x4-",
    "tags": []
   },
   "source": [
    "# Final) Test !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:38:10.816942Z",
     "iopub.status.busy": "2023-05-26T01:38:10.815444Z",
     "iopub.status.idle": "2023-05-26T01:38:10.828619Z",
     "shell.execute_reply": "2023-05-26T01:38:10.827364Z",
     "shell.execute_reply.started": "2023-05-26T01:38:10.816877Z"
    },
    "id": "LZRctQyl-0Lb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "from chatgpt.models.auto import AutoActor, AutoCritic\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "MAX_LEN = 256\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\":\n",
    "    (\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "     \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "     \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "     ),\n",
    "    \"prompt_no_input\":\n",
    "    (\"Below is an instruction that describes a task.\\n\"\n",
    "     \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "     \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-26T01:37:37.209772Z",
     "iopub.status.busy": "2023-05-26T01:37:37.208843Z",
     "iopub.status.idle": "2023-05-26T01:38:03.118836Z",
     "shell.execute_reply": "2023-05-26T01:38:03.117765Z",
     "shell.execute_reply.started": "2023-05-26T01:37:37.209704Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoActor(\n",
       "  (model): GPTNeoXForCausalLM(\n",
       "    (gpt_neox): GPTNeoXModel(\n",
       "      (embed_in): Embedding(30080, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (1): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (2): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (3): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (4): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (5): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (6): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (7): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (8): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (9): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (10): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (11): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (12): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (13): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (14): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (15): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (16): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (17): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (18): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (19): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (20): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (21): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (22): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "        (23): GPTNeoXLayer(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): GPTNeoXAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (mlp): GPTNeoXMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pretrain = 'KoAlpaca/train_v1.1b/polyglot-1.3b-koalpaca-v1.1b'\n",
    "model_directory = './output_3_PPO'\n",
    "model_path = os.path.join(model_directory, 'actor.pt')\n",
    "\n",
    "# configure model, tokenizer\n",
    "actor = AutoActor(pretrained=pretrain).to(torch.cuda.current_device())\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain,\n",
    "                                          padding_side=\"right\",\n",
    "                                          model_max_length=512)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "    \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "    \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "actor.model.load_state_dict(state_dict)\n",
    "\n",
    "actor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:38:16.157554Z",
     "iopub.status.busy": "2023-05-26T01:38:16.156408Z",
     "iopub.status.idle": "2023-05-26T01:38:31.544682Z",
     "shell.execute_reply": "2023-05-26T01:38:31.543770Z",
     "shell.execute_reply.started": "2023-05-26T01:38:16.157496Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 01:38:20.279311: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-26 01:38:20.319378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì œê°€ AI ì±—ë´‡ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì–´ìˆê¸° ë•Œë¬¸ì— ë¬¼ë¦¬ì ì¸ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í•œìš°ëŠ” í•œìš° ì¤‘ì—ì„œë„ ê³ ê¸‰ìœ¡ì¢…ìœ¼ë¡œ ìœ ëª…í•˜ë©°, ê·¸ ë§›ê³¼ ì˜ì–‘ì  ê°€ì¹˜ê°€ ë†’ì€ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ì¸ì •ë°›ê³  ìˆìŠµë‹ˆë‹¤.ì— ë”°ë¼ ê°€ê²©ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ í•œìš°ëŠ” í•œìš° ì¤‘ì—ì„œ ê°€ì¥ ë¹„ì‹¼ ê°€ê²©ëŒ€ì— ì†í•œë‹µë‹ˆë‹¤. ë”°ë¼ì„œ êµ¬ë§¤ ì „ì—ëŠ” ê°€ê²© ì •ë³´ë¥¼ ì¶©ë¶„íˆ í™•ì¸í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.!ë‹¤ì‚¬ì˜¤ì§€ ë§ˆì„¸ìš”.ì´ì˜ ë‹µë³€ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. :\\n\\n'ë„¤, í•œìš° ì¤‘ì—ì„œë„ ê³ ê¸‰\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1973ë…„ë¶€í„° 1974ë…„ê¹Œì§€ ë¶€í†µë ¹ì„ ì—­ì„í–ˆìŠµë‹ˆë‹¤.\\n\\ní•˜ì§€ë§Œ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì„ ìˆ˜í–‰í•œ ì •í™•í•œ ë…„ë„ëŠ” ì•Œë ¤ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì„ ìˆ˜í–‰í•œ í•´ëŠ” 1944ë…„ì´ì—ˆìŠµë‹ˆë‹¤.ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì„ ìˆ˜í–‰í•œ í•´ëŠ” ì •í™•íˆ ì•Œë ¤ì§€ì§€ ì•Šì•˜ì§€ë§Œ, ë¯¸êµ­ ë‚´ì—ì„œëŠ” 1950ë…„ëŒ€ì— ì´ë¥´ëŸ¬ ì´ ë¬¸ì œì— ëŒ€í•œ ì—°êµ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.ë‹‰ìŠ¨ì˜ ì´ë¦„ì´ ëˆ„êµ¬ì¸ì§€ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.ë‹‰ìŠ¨ì€\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì—ì„œ ë¶ìª½ìœ¼ë¡œ 1ì‹œê°„ ì •ë„ì˜ ê±°ë¦¬ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì—ì„œ ë™ìª½ìœ¼ë¡œ 1ì‹œê°„ ì •ë„ì˜ ê±°ë¦¬ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì—ì„œ ë‚¨ìª½ìœ¼ë¡œ 3ì‹œê°„ ì •ë„ì˜ ê±°ë¦¬ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì™€ ì¸ì ‘í•œ ê³³ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì—ì„œ ì„œìª½ìœ¼ë¡œ ì•½ 1ì‹œê°„ ì •ë„ì˜ ê±°ë¦¬ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì™€ ë‚¨ìª½ìœ¼ë¡œ 3ì‹œê°„ ì •ë„ì˜ ê±°ë¦¬ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì—ì„œ ë¶ìª½ìœ¼ë¡œ 1ì‹œê°„ ì •ë„ì˜\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "\n",
      "### Response(ì‘ë‹µ):' ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” ì „êµ­ì ìœ¼ë¡œ ë§¤ìš° ì‹¬ê°í•œ ìˆ˜ì¤€ì´ë©°, ë§ˆìŠ¤í¬ ì°©ìš©ê³¼ ì‹¤ì™¸ í™œë™ ìì œ ë“±ì„ ê¶Œì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì™¸ì¶œ ì‹œì—ëŠ” ë¯¸ì„¸ë¨¼ì§€ ë§ˆìŠ¤í¬ ì°©ìš©ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” ë¯¸ì„¸ë¨¼ì§€ ë†ë„ì™€ëŠ” ìƒê´€ì—†ì´ ë§¤ìš° ì‹¬ê°í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.\\n ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ í™•ì¸í•˜ê³  ë§ˆìŠ¤í¬ë¥¼ ì°©ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë¯¸ì„¸ë¨¼ì§€ ë†ë„ì— ëŒ€í•´ì„œëŠ” ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ í™•ì¸í•˜ê³ , ë¯¸ì„¸ë¨¼ì§€ ë§ˆìŠ¤í¬ë¥¼ ì°©ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ í™•ì¸í•˜ë ¤ë©´ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ ì¸¡ì •ê¸°ë¡œ ì¸¡ì •í•˜ê±°ë‚˜, ë¯¸ì„¸ë¨¼ì§€ ë†ë„ ì›¹ì‚¬ì´íŠ¸ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¯¸ì„¸ë¨¼ì§€\n"
     ]
    }
   ],
   "source": [
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=MAX_LEN,\n",
    "                             do_sample=True,\n",
    "                             top_k=20,\n",
    "                             top_p=0.95,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             repetition_penalty=2.0,\n",
    "                             num_return_sequences=1,\n",
    "                             early_stopping=True)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?', \n",
    "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?', \n",
    "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01e03037c3f74f6aafa140809ad78ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12f483b4282f45b8a72c7ff5d8cb71dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5e9abba2c124b7ea3c981da106f3765",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4ed1b179c0e54435af4bd2a9fba6ad43",
      "value": " 513M/513M [00:03&lt;00:00, 168MB/s]"
     }
    },
    "18e0f9550cec40a6b9ecf819c4807f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_538a5792df47419296ca36504f6b40d6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01e03037c3f74f6aafa140809ad78ef7",
      "value": "Downloading (â€¦)/main/tokenizer.json: 100%"
     }
    },
    "4778416a939b4caba8499487d8c245af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e85b4dd532c4d61adbef1fb0f110a7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64b52fa83f374aee8f4488a70314d142",
       "IPY_MODEL_698e16ed0dc34b53bf509380c0b78f84",
       "IPY_MODEL_b942d75f81ba490b887b1c6af9c8cb0a"
      ],
      "layout": "IPY_MODEL_e33d51e99c2f4e4896931afc1f9e959d"
     }
    },
    "4ed1b179c0e54435af4bd2a9fba6ad43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "532596024b1e44ae92935e6b82bc4c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "538a5792df47419296ca36504f6b40d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "541355aca55c42d49cfd3339e93e5d7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57abd7cd0b3841f6bddc3a82da477b23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "644f578cdc0e424cbb2af32aa8e2615b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57abd7cd0b3841f6bddc3a82da477b23",
      "max": 2825034,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2ea0160be5f44c6920a03725b9ebac3",
      "value": 2825034
     }
    },
    "64b52fa83f374aee8f4488a70314d142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1a26a1d1e9b4c8089e4a3d8ed5f2d21",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_88e82b137d9147c993da6853be21d2cf",
      "value": "Downloading (â€¦)lve/main/config.json: 100%"
     }
    },
    "698e16ed0dc34b53bf509380c0b78f84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab55e2eec5a5468894348ccef1d63650",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e232bfd4c35642e39dc5757a11e632cc",
      "value": 1000
     }
    },
    "6fda5850786948b398c192315f83534a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bdd205d9ea44b38a890ff006e90f5d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3266159dc3a41ba9c13cfb415e28cd2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8818c4943b1c4a0d9466b32db0ea2d42",
      "value": " 2.83M/2.83M [00:00&lt;00:00, 6.58MB/s]"
     }
    },
    "837d3d082ed049e29752b49aba9337b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b91f0ff6a8e642fd9fad16f3dfcea708",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4778416a939b4caba8499487d8c245af",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "8818c4943b1c4a0d9466b32db0ea2d42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88e82b137d9147c993da6853be21d2cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fe0939433504673958736bd77ca3b59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18e0f9550cec40a6b9ecf819c4807f6d",
       "IPY_MODEL_644f578cdc0e424cbb2af32aa8e2615b",
       "IPY_MODEL_7bdd205d9ea44b38a890ff006e90f5d9"
      ],
      "layout": "IPY_MODEL_e1a18cb3fe8d423b890df252c1a9e599"
     }
    },
    "91b5d937a4ff4f2385f74c43f320513f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5327f3d60504843af3a6dd7343be3ac",
      "max": 513302779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_541355aca55c42d49cfd3339e93e5d7a",
      "value": 513302779
     }
    },
    "a08557311b0740d6817ed5a511eb2298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a26a1d1e9b4c8089e4a3d8ed5f2d21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3850b935744426a80da042883eef72e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_837d3d082ed049e29752b49aba9337b2",
       "IPY_MODEL_91b5d937a4ff4f2385f74c43f320513f",
       "IPY_MODEL_12f483b4282f45b8a72c7ff5d8cb71dd"
      ],
      "layout": "IPY_MODEL_6fda5850786948b398c192315f83534a"
     }
    },
    "ab55e2eec5a5468894348ccef1d63650": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b91f0ff6a8e642fd9fad16f3dfcea708": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b942d75f81ba490b887b1c6af9c8cb0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a08557311b0740d6817ed5a511eb2298",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_532596024b1e44ae92935e6b82bc4c6b",
      "value": " 1.00k/1.00k [00:00&lt;00:00, 15.7kB/s]"
     }
    },
    "c3266159dc3a41ba9c13cfb415e28cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2ea0160be5f44c6920a03725b9ebac3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5327f3d60504843af3a6dd7343be3ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1a18cb3fe8d423b890df252c1a9e599": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e232bfd4c35642e39dc5757a11e632cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e33d51e99c2f4e4896931afc1f9e959d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5e9abba2c124b7ea3c981da106f3765": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
